{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those who know how data is structured, and have the tools for its processing RULE THE FUCKEN WORLD\n",
    "\n",
    "-- Hypothesis is an assumption of the value of a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Data Analysis Process\n",
    "\n",
    "1. Describe studied object\n",
    "2. Identify variables and formulate hypothesis\n",
    "3. Collect and prepare Data\n",
    "4. Exploratory Analysis\n",
    "5. Data cleaning, i.e. remove noise and outliers\n",
    "6. Data conversion\n",
    "7. Build model\n",
    "8. Interpret results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DESCRIBING OBJECTS\n",
    "\n",
    "Requires professional services to understand the nuances and details of the object of interest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. IDENTIFY VARIABLES & FORMULATE HYPOTHESIS & OPERATIONALIZE THE CONCEPTS\n",
    "\n",
    "We transition from abstract concepts of the domain of the variable, to measure them both quantitatively, & qualitatively\n",
    "\n",
    "E.g. Object Geek Radius Members.\n",
    "\n",
    "        first name: Mike\n",
    "        last name: Emono\n",
    "        age: 31\n",
    "        marital status: Single\n",
    "        profession: Multimedia Director\n",
    "\n",
    "        Has got gym membership? --> This can be a hypothesis\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. COLLECT & PREPARE DATA FOR HYPOTHESIS TESTING\n",
    "\n",
    "    Opinion polls\n",
    "    Observations\n",
    "    Documents\n",
    "    Direct measurements\n",
    "    Social media\n",
    "    External sources\n",
    "\n",
    "        Sample data is preprocessed before loading into storage. \n",
    "        Missing values are added, and credibility is checked before loading into storage befrore further analysis.    \n",
    "\n",
    "    Data sources: e3data, dataportals, data.gov.ru, academictorrents, \n",
    "    https://developer.ibm.com/technologies/artificial-intelligence/data/\n",
    "\n",
    "    PREPROCESSING DATA\n",
    "\n",
    "        Load data into storage\n",
    "        Data splitting\n",
    "        Data conversion to same unit of measurement\n",
    "        Unify data vocabulary\n",
    "        Merge data from multiple sources\n",
    "        Combine data from different sources\n",
    "        Add missing numeric values\n",
    "        Clean data; remove duplicates, check ranges, and regular expressions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. EXPLORATORY ANALYSIS\n",
    "\n",
    "    The goal is to immerse data, reveal basic structures, select most important variables, detect outliers/anomalies, and test basic hypothesis.\n",
    "\n",
    "    1. Analyse main properties of data\n",
    "    2. Find patterns\n",
    "    3. Find distribution\n",
    "    4. Select most important variables\n",
    "    5. Detect outliers and anomalies\n",
    "    6. Build initial model if possible.\n",
    "    7. Test some basic hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. CLEAN DATA - REMOVE NOISE & ANOMALIES\n",
    "\n",
    "    1. Data conversion for consistency\n",
    "    2. Normalize variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. BUILD MODELS\n",
    "\n",
    "    Testing the hypothesis and building mathematical models for our variable which describe the behavior & relationships between variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. INTERPRETATION\n",
    "\n",
    "    A process of transforming data into meaning information that is useful\n",
    "\n",
    "        Depends on point of view\n",
    "        Could be informal\n",
    "        This final result represents expert knowledge that can be, and should be scaled/reproduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe objects.\n",
    "\n",
    "1. Continuous\n",
    "2. Discrete\n",
    "3. Geospacial\n",
    "4. Logical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify variables from Population\n",
    "Formulate Hypothesis -> Assumptions of variables\n",
    "\n",
    "Nominals can be aggregated; e.g. 30% Deli products\n",
    "Variables could be nominal, ordinal scale, intervals or ratios\n",
    "Ordinals scale depics qualitative values like ratings (1->strongly disagreee, 2->disagree, etc)\n",
    "Interval scale with arbitrary starting point, or 0 point, e.g. chronology of the starting point of the creation, temparature\n",
    "Ratio scale, has absolute zero as starting point, representing the absence of the variable being measured. E.g. speed of car, number of apples in fridge, product price, \n",
    "\n",
    "\n",
    "1. Factoids\n",
    "2. Series - Target variable mapped to predictors. E.g when time is  an independent variable, Time series; total visits in 2020, 2021, & 2022\n",
    "3. Table - Several units of dependent/target information, and one unit of independent/predictor information.\n",
    "4. Transactions - Raw records of data describing events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect & preprocess data before loading to storage; storage structure, missing values, credibility\n",
    "\n",
    "Data sources: \n",
    "opinion polls, -> compreshensive & mutually exclusive options, e.g. Intervals. Google Forms\n",
    "observations, -> usually outsourced, subjective judgement, human error, e.g. data about animals, plants, traffic flow,\n",
    "documents, -> structured documents like hand written medical reports, \n",
    "results of direct measurements, -> raw transactions with no aggregation, e.g. flightredirect24\n",
    "social media, -> APIs\n",
    "external sources -> , Search engines, Wikipedia, Open Datastorages (re3data, dataportals, data.gov.ru, academictorrents, ), Government DBs, Institutionl repositories, \n",
    "\n",
    "Preprocessing.\n",
    "\n",
    "1. Load into storage; remove unreadable characters, load as text fields, \n",
    "2. Split data; separating fname & lnames, addresses; e.g. use a dictionary of common names to analyze fields\n",
    "3. Bring data into same units of measurements; e.g. lbs -> kgs\n",
    "4. Convert to unified vocabulary; Eng, english, English language -> English; convert to one case, association list, convert data.\n",
    "5. Merge data from multiple sources; make sure data structure is the same; merge, identify, rename for consistency\n",
    "6. Combining data from different sources; investigate matchine fields, find shared attributes\n",
    "7. Adding missing numerical values; add mean, average or central mean, for time series, nearest neighbours\n",
    "8. Data cleaning; Remove duplicates, check ranges, and compare with references or regular expressions, range check / outliers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorative data analysis; Representative Sample -> Distribution, reveal structures, select important variables\n",
    "detect outliers, noise, and analysis, test basic hypothesis\n",
    "\n",
    "Description >> Mean, Mode, Median, Range/Quartiles, Variance, Standard Deviation\n",
    "1. Box plot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning;  remove noise & anomalies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation for variables with multiple distinct value; e.g. usd, dollars, $.\n",
    "Normalization\n",
    "1. ln(x)\n",
    "2. log(x)\n",
    "3. Sqrt(x)\n",
    "4. Square(x)\n",
    "5. 1/x\n",
    "\n",
    "Normaliztion - adjust parameters to one scale, & one range\n",
    "1. Linear normalization >> y(X) = (X - Xmin)/(Xmax - Xmin)\n",
    "2. Exponential normalization >> y(X) = 1 - EXP(1 - (X/Xmin))\n",
    "\n",
    "Types of parameters\n",
    "1. Unipolar parameters - degree of quantity or quality (linear normalized in range 0, 1) >> y(X) = (X - Xmin)/(Xmax - Xmin)\n",
    "2. Bipolar parameters - not just quantity or quality, but direction (linear normalized in range of -1, 1) >> y(X) = (2X - (Xmax + Xmin))/(Xmax - Xmin)\n",
    "\n",
    "Objective function based on normalized values\n",
    "- Mathmatical criteria for object's quality (process, solution), set to obtain one parameter, instead of a large number of qualitative parameters for each studied object\n",
    "- Used to find the object at which the desired outcome is obtained, based on the maximum, or minimum of the function.\n",
    "- E.g. If objective function reflects cumulative positive qualities of a student, then max\n",
    "- E.g. If objective function reflects costs, then min\n",
    "- In the grade example, if we have 4 grade variables; so Objective function = normalized avg grade(X) + normalized certificate count (X)... Student with max of function wins\n",
    " - We can add new column; sum of normalized objective functions, or stack column chart it\n",
    " - We can also increase the weights of resulting normalized average grade, by multiplying by 1.x or 2 or whatever \n",
    "\n",
    " The process\n",
    " \n",
    " Object = Feature 1, Feature 2, Feature 3, ... Feature n\n",
    "\n",
    " Objective function is numerical function of several features that is optimized (minimized, or maximized) to solve an optimization problem\n",
    "\n",
    " 1. Normalize features\n",
    " 2. Construct objective function based on task\n",
    " 3. Find the largest or smallest value of the objective function on the dataset. AKA global optimum/maximum or minimum\n",
    "\n",
    " F(X) = a1.x1 + a2.x2 + a3.x3 + ...an.xn\n",
    "\n",
    "   Where  ai = coefficient or weight\n",
    "   and xi = normalized parameters\n",
    "\n",
    "    Transforming norminal data; e.g. Color = [{1, 'blue'}, {2, 'red'}, {3, 'green'}] \n",
    "    >> To numeric data \n",
    "             blue  red  green\n",
    "        1     1     0     0\n",
    "        2     0     1     0\n",
    "        3     0     0     1\n",
    "Ordinal data is easier, whether bipolar or unipolar, we can match each item to a numeric value on a numerical scale\n",
    "\n",
    "Example; \n",
    "\n",
    "1. What do we now about a car we want to rent (features)\n",
    "\n",
    "a. fuel type\n",
    "b. gearbox type\n",
    "c. heated steering wheel\n",
    "d. trunk size\n",
    "e. the year of manufacture\n",
    "f. daily rental price\n",
    "\n",
    "2. How do we choose the best car\n",
    "\n",
    "a. First, tranform data to numeric values\n",
    "        Fuel type, nominal dichotomous data, diesel or gasoline >> gasoline = 0, and diesel = 1\n",
    "        heated stearing wheel is dichotomous data >> yes or no >> 1 or 0. No further normlaization requried\n",
    "        Gearbox type could be automatic, robotic, or manual, which can be ordinally represented numerically >> {1, 'manual'}, {2, 'robotic'}, {3, 'automatic'}\n",
    "\n",
    "b. Normalize these values\n",
    "        Fuel type >> {0, 'gasoline'}, {1, 'diesel'}\n",
    "        Heated steering wheel >> {0, 'no'}, {1, 'yes'}\n",
    "        Normalized Gearbox type >> {0, 'manual'}, {0.5, 'robotic'}, {1, 'automatic'}\n",
    "        Volume of car trunk's data is numerical >> Linear normalize (trunk size)\n",
    "        Year of manufacture is also numerical >> Linear Normalize(year)\n",
    "        Rental price is also numerical >> Linear Normalize(price)\n",
    "\n",
    "c. Construct objective function, and use its extremum (minimum or maximum) to choose best car\n",
    "        It's important to consult field experts to determine which feature is most important and why\n",
    "        In this example, we're experts\n",
    "\n",
    "        Assumption;\n",
    "\n",
    "                1. All features are equally important to us (no weights/coefficients)\n",
    "                2. We prefer diesel car to petrol car (max normalized value)\n",
    "                3. Gear box preference; automatic, robotic, and manual (max normalized value)\n",
    "                4. Heated seat is desireable (max normalized value)\n",
    "                5. Prefer larget trunk (max normalized value)\n",
    "                6. A newer car is better (max normalized value)\n",
    "                7. Rental price, we prefer lowest rental price (min normalized value)\n",
    "\n",
    "        We can combine these expert opinions, to get the objective function\n",
    "\n",
    "        F(X) = NF(X) + NB(X) + NH(X) + NV(X) + NY(X) + (1 - NP(X))\n",
    "\n",
    "        Where \n",
    "        NF(X) is Normalized value corresponding to the type of fuel\n",
    "        NB(X) is noormalized value corresponding to gearbox type\n",
    "        NH(X) ... heated steering wheel\n",
    "        NV(X) ... volume of trunk\n",
    "        NY(X) ... year of manufacture\n",
    "        NP(X) ... rental price\n",
    "\n",
    "        Note to indicate minimized normalized value, we subtract normalized value from 1, preferring car with lowest cost\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build mathematical models that describe the behavior of the variables and relationships that exist between them.\n",
    "Test hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferences/Interpretation. Transform data into meaningful information; point of view"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store with ARANGO DB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize with TABLEAU\n",
    "\n",
    "Find dependencies, distribution, composition & relationship\n",
    "\n",
    "1. Idea illustration: Org chart, Business Process\n",
    "2. Idea generation: Mind map\n",
    "3. Visual study: Find patterns in multivariate distributions\n",
    "4. Routine visualization: Reports\n",
    "5. Concept visualization: develop complex ideas & plans with concept maps, graphs, gantt chart, etc\n",
    "6. Strategic visualization: Performance chart, life cycle diagram, org chargs\n",
    "7. Metaphorical visualization: Metro maps\n",
    "8. Combined visualization; puts several complex charts into one, like weather map\n",
    "\n",
    "Techniques:\n",
    "\n",
    "1. Multivariate data representation; parallel coordinates, radar charts, & Chernoff Faces \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
